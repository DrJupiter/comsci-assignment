{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502b4822",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76ab67",
   "metadata": {},
   "source": [
    "_Exercise: Creating Word Shifts_\n",
    ">    1. Pick a day of your choice in 2020. We call it $d$. It is more interesting if you pick a day where you expect something relevant to occur (e.g. Christmas, New Year, Corona starting, the market crashes...).\n",
    ">    2. Build two lists $l$ and $l_{ref}$ containing all tokens for submissions posted on r/wallstreebets on day $d$, and in the 7 days preceding day $d$, respectively. \n",
    ">    3. For each token $i$, compute the relative frequency in the two lists $l$ and $l_{ref}$. We call them $p(i,l)$ and $p(i,l_{ref})$, respectively. The relative frequency is computed as the number of times a token occurs over the total length of the document. Store the result in a dictionary.\n",
    ">    4. For each token $i$, compute the difference in relative frequency $\\delta p(i) = p(i,l) - p(i,l_{ref})$. Store the values in a dictionary. Print the top 10 tokens (those with largest relative frequency). Do you notice anything interesting?\n",
    ">    5. Now, for each token, compute the happiness $h(i) = labMT(i) - 5$, using the labMT dictionary. Here, we subtract $5$, so that positive tokens will have a positive value and negative tokens will have a negative value. Then, compute the product $\\delta \\Phi = h(i)\\cdot \\delta p(i)$. Store the results in a dictionary. \n",
    ">    6. Print the top 10 tokens, ordered by the absolute value of $|\\delta \\Phi|$. Explain in your own words the meaning of $\\delta \\Phi$. If that is unclear, have a look at [this page](https://shifterator.readthedocs.io/en/latest/cookbook/weighted_avg_shifts.html).\n",
    ">    7. Now install the [``shifterator``](https://shifterator.readthedocs.io/en/latest/installation.html) Python package. We will use it for plotting Word Shifts. \n",
    ">    8. Use the function ``shifterator.WeightedAvgShift`` to plot the WordShift, showing which words contributed the most to make your day of choice _d_ happier or more sad then days in the preceding 7 days. Comment on the figure. \n",
    ">    9. How do words that you printed in step 6 relate to those shown by the WordShift? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "df545e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdate\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import datetime as datetime\n",
    "\n",
    "data = pd.read_csv(\"wallstreet_subs.csv\", parse_dates = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "01da9227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1586173811</td>\n",
       "      <td>What is the Fed actually buying?</td>\n",
       "      <td>Okay, I may actually just be retarded. On my d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1586173320</td>\n",
       "      <td>I didn’t learn about puts because I was lazy</td>\n",
       "      <td>Beginning of the this virus shit, everyone was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1586173268</td>\n",
       "      <td>HOT TAKE</td>\n",
       "      <td>Literally everyone has free time on their hand...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1586172639</td>\n",
       "      <td>Fuck you Gordon</td>\n",
       "      <td>Gordon I believed in you, I can't even begin t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1586171822</td>\n",
       "      <td>Can’t find a picture</td>\n",
       "      <td>Someone uploaded a ohoto of the stock market h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc                                         title  \\\n",
       "0   1586173811              What is the Fed actually buying?   \n",
       "1   1586173320  I didn’t learn about puts because I was lazy   \n",
       "2   1586173268                                      HOT TAKE   \n",
       "3   1586172639                               Fuck you Gordon   \n",
       "4   1586171822                          Can’t find a picture   \n",
       "\n",
       "                                            selftext  score  \n",
       "0  Okay, I may actually just be retarded. On my d...      1  \n",
       "1  Beginning of the this virus shit, everyone was...      1  \n",
       "2  Literally everyone has free time on their hand...      1  \n",
       "3  Gordon I believed in you, I can't even begin t...      1  \n",
       "4  Someone uploaded a ohoto of the stock market h...      1  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a19c51",
   "metadata": {},
   "source": [
    "Combining titles and main text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c16d65d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = []\n",
    "for i in range(len(data['title'])):\n",
    "    empty.append(data['title'][i]+' '+data['selftext'][i])\n",
    "data['text'] = empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f83182",
   "metadata": {},
   "source": [
    ">    1. Pick a day of your choice in 2020. We call it $d$. It is more interesting if you pick a day where you expect something relevant to occur (e.g. Christmas, New Year, Corona starting, the market crashes...).\n",
    "\n",
    "We pick November 4rd as d, which is the day after election day of the US presidential election.\n",
    "\n",
    "OR\n",
    "\n",
    "We pick March 13th as d, which is the day the president delcared a state of emergency in the country over the Corona virus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b5b45",
   "metadata": {},
   "source": [
    "Converting utc to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "02717f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = [datetime.datetime.fromtimestamp(ts) for ts in data['created_utc']]\n",
    "data['date'] = data['date'].dt.date\n",
    "data['date'] = pd.to_datetime(data['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "4d5073d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "c985ee9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604382582</td>\n",
       "      <td>Calling it now</td>\n",
       "      <td>Trump wins...\\nMarket crashes as the party of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Calling it now Trump wins...\\nMarket crashes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604381964</td>\n",
       "      <td>Turnaround day for COLM</td>\n",
       "      <td>Fellow Autists:\\nCOLM had a trash earnings cal...</td>\n",
       "      <td>1</td>\n",
       "      <td>Turnaround day for COLM Fellow Autists:\\nCOLM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604381864</td>\n",
       "      <td>Guaranteed 100% gains, don't miss out you Ret....</td>\n",
       "      <td>Many of you R words have provably not even hea...</td>\n",
       "      <td>1</td>\n",
       "      <td>Guaranteed 100% gains, don't miss out you Ret....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604381141</td>\n",
       "      <td>BREAKING ELECTION NEWS</td>\n",
       "      <td>BREAKING: Less than 24 hours before Election D...</td>\n",
       "      <td>1</td>\n",
       "      <td>BREAKING ELECTION NEWS BREAKING: Less than 24 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604380951</td>\n",
       "      <td>SHORT SELLERS HAVE TO COVER, PUMP INCOMING GNCA</td>\n",
       "      <td>hey fellow cigarettes,\\n\\nHere's a good tip fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>SHORT SELLERS HAVE TO COVER, PUMP INCOMING GNC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604426628</td>\n",
       "      <td>since there's a lot of dead bears out today,I ...</td>\n",
       "      <td>https://youtu.be/wMxKbkWrvDc</td>\n",
       "      <td>1</td>\n",
       "      <td>since there's a lot of dead bears out today,I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604426482</td>\n",
       "      <td>Is anyone buying $NEE here?!</td>\n",
       "      <td>A big IF since no one knows who wins tonight e...</td>\n",
       "      <td>1</td>\n",
       "      <td>Is anyone buying $NEE here?! A big IF since no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604425864</td>\n",
       "      <td>$BHC Earnings Call Jitters?</td>\n",
       "      <td>BHC was up 6% after it beat revenue and profit...</td>\n",
       "      <td>1</td>\n",
       "      <td>$BHC Earnings Call Jitters? BHC was up 6% afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604425400</td>\n",
       "      <td>A warning ⚠️ for my beloved autist’s</td>\n",
       "      <td>I know I know take a look at this guy is what ...</td>\n",
       "      <td>1</td>\n",
       "      <td>A warning ⚠️ for my beloved autist’s I know I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>1604424178</td>\n",
       "      <td>Shenanigans in China - ANT Group IPO rejected,...</td>\n",
       "      <td>China called a sudden halt to the world’s[ big...</td>\n",
       "      <td>1</td>\n",
       "      <td>Shenanigans in China - ANT Group IPO rejected,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_utc                                              title  \\\n",
       "date                                                                         \n",
       "2020-11-03   1604382582                                     Calling it now   \n",
       "2020-11-03   1604381964                            Turnaround day for COLM   \n",
       "2020-11-03   1604381864  Guaranteed 100% gains, don't miss out you Ret....   \n",
       "2020-11-03   1604381141                             BREAKING ELECTION NEWS   \n",
       "2020-11-03   1604380951    SHORT SELLERS HAVE TO COVER, PUMP INCOMING GNCA   \n",
       "...                 ...                                                ...   \n",
       "2020-11-03   1604426628  since there's a lot of dead bears out today,I ...   \n",
       "2020-11-03   1604426482                       Is anyone buying $NEE here?!   \n",
       "2020-11-03   1604425864                        $BHC Earnings Call Jitters?   \n",
       "2020-11-03   1604425400               A warning ⚠️ for my beloved autist’s   \n",
       "2020-11-03   1604424178  Shenanigans in China - ANT Group IPO rejected,...   \n",
       "\n",
       "                                                     selftext  score  \\\n",
       "date                                                                   \n",
       "2020-11-03  Trump wins...\\nMarket crashes as the party of ...      1   \n",
       "2020-11-03  Fellow Autists:\\nCOLM had a trash earnings cal...      1   \n",
       "2020-11-03  Many of you R words have provably not even hea...      1   \n",
       "2020-11-03  BREAKING: Less than 24 hours before Election D...      1   \n",
       "2020-11-03  hey fellow cigarettes,\\n\\nHere's a good tip fo...      1   \n",
       "...                                                       ...    ...   \n",
       "2020-11-03                       https://youtu.be/wMxKbkWrvDc      1   \n",
       "2020-11-03  A big IF since no one knows who wins tonight e...      1   \n",
       "2020-11-03  BHC was up 6% after it beat revenue and profit...      1   \n",
       "2020-11-03  I know I know take a look at this guy is what ...      1   \n",
       "2020-11-03  China called a sudden halt to the world’s[ big...      1   \n",
       "\n",
       "                                                         text  \n",
       "date                                                           \n",
       "2020-11-03  Calling it now Trump wins...\\nMarket crashes a...  \n",
       "2020-11-03  Turnaround day for COLM Fellow Autists:\\nCOLM ...  \n",
       "2020-11-03  Guaranteed 100% gains, don't miss out you Ret....  \n",
       "2020-11-03  BREAKING ELECTION NEWS BREAKING: Less than 24 ...  \n",
       "2020-11-03  SHORT SELLERS HAVE TO COVER, PUMP INCOMING GNC...  \n",
       "...                                                       ...  \n",
       "2020-11-03  since there's a lot of dead bears out today,I ...  \n",
       "2020-11-03  Is anyone buying $NEE here?! A big IF since no...  \n",
       "2020-11-03  $BHC Earnings Call Jitters? BHC was up 6% afte...  \n",
       "2020-11-03  A warning ⚠️ for my beloved autist’s I know I ...  \n",
       "2020-11-03  Shenanigans in China - ANT Group IPO rejected,...  \n",
       "\n",
       "[169 rows x 5 columns]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc['2020-11-03']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a104131",
   "metadata": {},
   "source": [
    ">    2. Build two lists $l$ and $l_{ref}$ containing all tokens for submissions posted on r/wallstreebets on day $d$, and in the 7 days preceding day $d$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "533bde33",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.loc['2020-03-13'].index[0]\n",
    "d_min = d - datetime.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "ed8682be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample = remove_URL(sample)\n",
    "    sample = replace_contractions(sample)\n",
    "    # Tokenize\n",
    "    words = nltk.word_tokenize(sample)\n",
    "\n",
    "    # Normalize\n",
    "    return normalize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "6306fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['tokens'] = [preprocess(data['text'][i]) for i in range(len(data['text']))]\n",
    "\n",
    "#^takes a long time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "751e272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.ndarray.tolist(np.concatenate(data.loc[d]['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ccefa88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ref = []\n",
    "date = d_min\n",
    "while date <= d:\n",
    "    l_ref.append(np.concatenate(data.loc[date]['tokens']))\n",
    "    date += datetime.timedelta(days=1)\n",
    "l_ref = [item for sublist in l_ref for item in sublist]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61dbfd",
   "metadata": {},
   "source": [
    ">    3. For each token $i$, compute the relative frequency in the two lists $l$ and $l_{ref}$. We call them $p(i,l)$ and $p(i,l_{ref})$, respectively. The relative frequency is computed as the number of times a token occurs over the total length of the document. Store the result in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "2239527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "bb39d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dict([(item[0],item[1]/len(l)) for item in Counter(l).items()])\n",
    "p_ref = dict([(item[0],item[1]/len(l_ref)) for item in Counter(l_ref).items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "2a4abf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('puts', 0.011876339149066422),\n",
       " ('going', 0.00945821854912764),\n",
       " ('market', 0.008172635445362718),\n",
       " ('people', 0.00655035200489746),\n",
       " ('get', 0.006213651668197122),\n",
       " ('like', 0.006091215182124273),\n",
       " ('buy', 0.005693296602387512),\n",
       " ('go', 0.004897459442913988),\n",
       " ('spy', 0.004897459442913988),\n",
       " ('would', 0.0047444138353229266)]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(p.items(), key = lambda x:x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "dc8645c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('puts', 0.008769190874114216),\n",
       " ('going', 0.007578443772921882),\n",
       " ('market', 0.007266203421942559),\n",
       " ('like', 0.005837306900511757),\n",
       " ('get', 0.005609741898950555),\n",
       " ('people', 0.005302793757309864),\n",
       " ('amp', 0.005011722243685071),\n",
       " ('buy', 0.004731235148737543),\n",
       " ('would', 0.004387241541726424),\n",
       " ('time', 0.004291981773631038)]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(p_ref.items(), key = lambda x:x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f5fb7",
   "metadata": {},
   "source": [
    ">    4. For each token $i$, compute the difference in relative frequency $\\delta p(i) = p(i,l) - p(i,l_{ref})$. Store the values in a dictionary. Print the top 10 tokens (those with largest relative frequency). Do you notice anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "780ad97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = set(p.keys()).union(set(p_ref.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "76f1d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = dict([(token, p.get(token,0) - p_ref.get(token,0)) for token in all_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "e42a5089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('puts', 0.0031071482749522056),\n",
       " ('going', 0.0018797747762057584),\n",
       " ('trump', 0.0017760494170471043),\n",
       " ('today', 0.0013283633348227172),\n",
       " ('fucking', 0.0013010406639645039),\n",
       " ('next', 0.0012511225108896443),\n",
       " ('people', 0.0012475582475875956),\n",
       " ('weekend', 0.0009784937069499647),\n",
       " ('even', 0.0009724952217399995),\n",
       " ('buy', 0.0009620614536499685)]"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dp.items(), key = lambda x:x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7670f3",
   "metadata": {},
   "source": [
    "Notice anything interesting??\n",
    "\n",
    "Not really...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22b5eb",
   "metadata": {},
   "source": [
    ">    5. Now, for each token, compute the happiness $h(i) = labMT(i) - 5$, using the labMT dictionary. Here, we subtract $5$, so that positive tokens will have a positive value and negative tokens will have a negative value. Then, compute the product $\\delta \\Phi = h(i)\\cdot \\delta p(i)$. Store the results in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "168c9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "labMt_dict = pd.read_csv(\"Hedonometer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "a0b2d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "labMt_dict = labMt_dict.set_index(\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "72821ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.42"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labMt_dict.loc[\"love\"][\"Happiness Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "76de419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.42\n"
     ]
    }
   ],
   "source": [
    "print(labMt_dict[\"Happiness Score\"].get(\"love\",np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "2ab643a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = dict([(token, labMt_dict[\"Happiness Score\"].get(token,np.nan)-5) for token in all_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "fa68f807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "22ed15d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tops'"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(h, key = h.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "91d5a343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[\"tops\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "c50bc939",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'happy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'happy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15216/3119081161.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabMt_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"happy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'happy'"
     ]
    }
   ],
   "source": [
    "labMt_dict[\"happy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5991f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
